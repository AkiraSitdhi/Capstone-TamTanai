{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dir(directory):\n",
    "    contents = []\n",
    "    paths = []\n",
    "    for file_path in tqdm(glob(os.path.join(directory, '**/*.txt'), recursive=True)):\n",
    "        with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            contents.append(content)\n",
    "            paths.append(file_path)\n",
    "    return contents, paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analyze our legal document length distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = read_dir(\"_ตรวจแล้ว\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(content) for content in contents]\n",
    "paragraphs = [len(content.split(\"\\n\\n\")) for content in contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(lengths, bins=30, alpha=0.7, label='Length of strings')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of String Lengths')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the distribution of split counts\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(paragraphs, bins=30, alpha=0.7, color='orange', label='Count of \"\\\\n\\\\n\" splits')\n",
    "plt.xlabel('Count of \"\\\\n\\\\n\" splits')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of \"\\\\n\\\\n\" Splits in Strings')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame\n",
    "df = pd.DataFrame({'content': contents})\n",
    "df['length'] = df['content'].apply(len)\n",
    "df['split_count'] = df['content'].apply(lambda x: x.count(\"\\n\\n\"))\n",
    "\n",
    "# Compute quantiles for split counts\n",
    "split_count_quantiles = df['split_count'].quantile([0.25, 0.5, 0.75]).tolist()\n",
    "\n",
    "# Define bins and labels based on quantiles\n",
    "split_bins = [-1] + split_count_quantiles + [df['split_count'].max()]\n",
    "split_labels = [f\"{int(split_bins[i]) + 1} to {int(split_bins[i+1])}\" for i in range(len(split_bins)-1)]\n",
    "\n",
    "# Categorizing data into bins\n",
    "df['split_count_bin'] = pd.cut(df['split_count'], bins=split_bins, labels=split_labels, include_lowest=True)\n",
    "\n",
    "# Aggregating bin counts for visualization\n",
    "split_count_bin_counts = df['split_count_bin'].value_counts().reindex(split_labels)\n",
    "\n",
    "# Visualizing the distribution of \"\\n\\n\" split counts by detailed bins\n",
    "plt.figure(figsize=(10, 6))\n",
    "split_count_bin_counts.plot(kind='bar', color='orange')\n",
    "plt.title('Distribution of \"\\\\n\\\\n\" Split Counts by Detailed Bins')\n",
    "plt.xlabel('Split Count Range')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n(n):\n",
    "    if n<5: return 3\n",
    "    elif n<8: return 5\n",
    "    elif n<13: return 8\n",
    "    elif n<17: return 10\n",
    "    else: return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = 0\n",
    "for p in paragraphs:\n",
    "    n = get_n(p)\n",
    "    ns += n\n",
    "print(\"total estimated questions:\", ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generating dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n(n):\n",
    "    if n<5: return 3\n",
    "    elif n<8: return 5\n",
    "    elif n<13: return 8\n",
    "    elif n>18: return 14\n",
    "    else: return 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt(\n",
    "    messages,\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "):\n",
    "    \"\"\"\n",
    "    messages: list of dict\n",
    "\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"system instruction here\"},\n",
    "        {\"role\": \"user\", \"content\": \"user instruction here with inputs\"}\n",
    "    ]\n",
    "\n",
    "    model: str\n",
    "\n",
    "    openai model name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        messages = messages\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            # default settings\n",
    "            temperature=0,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            # stop=stop_sequence,\n",
    "        )\n",
    "        return True, response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        return False, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_TEMPLATE = \"\"\"Act as a dataset generator.\n",
    "Generate {n} legal question and answer pairs dataset based on the legal facts in a #given knowledge# section by following this requirements:\n",
    "- The generated contents must be in Thai language.\n",
    "- Don't use the same phrase/wording directly from #given knowledge# in a generated question.\n",
    "- Ensure that all text are clear and unambiguous. Specifically, avoid using phrases like \"เอกสารนี้ (this document)\", pronouns or similar constructs where the object of reference is not explicitly stated.\n",
    "- Use full reference to the law, for examples, \"มาตรา 1 ของพรบ.คอมพิวเตอร์\" instead of \"มาตรา 1\"\n",
    "- The question must be one of the following types:\n",
    "    (1) Asking for definition or declaration, for examples, \"มาตราที่ 1 ของพรบ.คอมพิวเตอร์เกี่ยวกับอะไร\"\n",
    "    (2) Situation based, for examples, \"นายสมชายต่อยเพื่อน ผิดกฎหมายข้อไหน\"\n",
    "    (3) Seeking advice, for examples, \"ถ้าเราโดนโกงเงินจากบัญชีธนาคารเราจะทำอย่างไรดี\"\n",
    "- The answer must also include references, for examples, \"อ้างอิงจากมาตรา 1,2 และ 4 ของพรบ.คอมพิวเตอร์\"\n",
    "- You must think in a following way: Issue, Rule, Application, Conclusion (IRAC).\n",
    "\"\"\"\n",
    "\n",
    "INPUT_TEMPLATE = \"\"\"\n",
    "#given knowledges#\n",
    "{knowledge}\n",
    "\"\"\"\n",
    "\n",
    "RESPONSE_TEMPLATE = \"\"\"\n",
    "The response must include question, its references from #given knowledges# and the answer in a JSON format as following:\n",
    "{{\n",
    "    1: {{\n",
    "        question: \"...\",\n",
    "        references: \"...\",\n",
    "        answer: \"...\"\n",
    "    }},\n",
    "    ...\n",
    "}}\n",
    "\n",
    "Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content(content_path_tuple):\n",
    "    content, path = content_path_tuple\n",
    "    n = get_n(len(content.split(\"\\n\\n\")))\n",
    "    instruction = (INSTRUCTION_TEMPLATE.format(n=n)) + (INPUT_TEMPLATE.format(knowledge=content)) + RESPONSE_TEMPLATE\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "    ]\n",
    "    isSuccess, response = ask_gpt(messages, model=\"gpt-4-0125-preview\")\n",
    "    if isSuccess:\n",
    "        return (response, path)\n",
    "    else:\n",
    "        return (None, path)\n",
    "    \n",
    "def run_in_parallel(contents, paths, num_threads=2):\n",
    "    # Combine contents and paths into a list of tuples for easier management\n",
    "    content_path_tuples = list(zip(contents, paths))\n",
    "    \n",
    "    results = [None] * len(contents)  # Pre-allocate list for results to maintain order\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        # Use a dictionary to keep track of futures and their corresponding index\n",
    "        future_to_index = {executor.submit(process_content, cpt): i for i, cpt in enumerate(content_path_tuples)}\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_index)):\n",
    "            index = future_to_index[future]\n",
    "            response, path = future.result()\n",
    "            # Use the index to place the result in the correct position\n",
    "            results[index] = (response, path)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 4978.40it/s]\n",
      "6it [05:32, 55.46s/it] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>references</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>พระราชบัญญัติว่าด้วยราคาสินค้าและบริการ พ.ศ. 2...</td>\n",
       "      <td>มีวัตถุประสงค์เพื่อปรับปรุงกฎหมายว่าด้วยการกำห...</td>\n",
       "      <td>หมายเหตุจากพระราชบัญญัติว่าด้วยราคาสินค้าและบร...</td>\n",
       "      <td>_ตรวจแล้ว/พระราชบัญญัติว่าด้วยราคาสินค้าและบริ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>การจำหน่ายตามพระราชบัญญัติว่าด้วยราคาสินค้าและ...</td>\n",
       "      <td>หมายถึงการขาย, แลกเปลี่ยน, ให้, จ่ายแจก, โอนสิ...</td>\n",
       "      <td>มาตรา 4 ของพระราชบัญญัติว่าด้วยราคาสินค้าและบร...</td>\n",
       "      <td>_ตรวจแล้ว/พระราชบัญญัติว่าด้วยราคาสินค้าและบริ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  พระราชบัญญัติว่าด้วยราคาสินค้าและบริการ พ.ศ. 2...   \n",
       "1  การจำหน่ายตามพระราชบัญญัติว่าด้วยราคาสินค้าและ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  มีวัตถุประสงค์เพื่อปรับปรุงกฎหมายว่าด้วยการกำห...   \n",
       "1  หมายถึงการขาย, แลกเปลี่ยน, ให้, จ่ายแจก, โอนสิ...   \n",
       "\n",
       "                                          references  \\\n",
       "0  หมายเหตุจากพระราชบัญญัติว่าด้วยราคาสินค้าและบร...   \n",
       "1  มาตรา 4 ของพระราชบัญญัติว่าด้วยราคาสินค้าและบร...   \n",
       "\n",
       "                                              source  \n",
       "0  _ตรวจแล้ว/พระราชบัญญัติว่าด้วยราคาสินค้าและบริ...  \n",
       "1  _ตรวจแล้ว/พระราชบัญญัติว่าด้วยราคาสินค้าและบริ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 45\n"
     ]
    }
   ],
   "source": [
    "legal_name = \"พระราชบัญญัติว่าด้วยราคาสินค้าและบริการ\"\n",
    "contents, paths = read_dir(f\"_ตรวจแล้ว/{legal_name}\")\n",
    "results = run_in_parallel(contents, paths, num_threads=4)\n",
    "\n",
    "df = pd.DataFrame(columns=['question', 'answer', 'references', 'source'])\n",
    "\n",
    "metadata = {}\n",
    "for i in range(len(contents)):\n",
    "    metadata[paths[i]] = {\"expected\": get_n(len(contents[i].split(\"\\n\\n\"))), \"actual\":0}\n",
    "\n",
    "for response_str, path in results:\n",
    "    if response_str:\n",
    "        start_index = response_str.find('{')\n",
    "        end_index = response_str.rfind('}') + 1\n",
    "        if start_index != -1 and end_index != -1: json_str = response_str[start_index:end_index]\n",
    "        else:\n",
    "            raise Exception(\"JSON not found\")\n",
    "        try:\n",
    "            response = json.loads(json_str)\n",
    "            # print(response_str[8:-4])\n",
    "            counts = 0\n",
    "            for key, value in response.items():\n",
    "                # Extract each question, answer, and references, and add them to the DataFrame\n",
    "                df.loc[len(df)] = [value['question'], value['answer'], value['references'], path]\n",
    "                counts += 1\n",
    "            metadata[path][\"actual\"] = counts\n",
    "        except Exception as e:\n",
    "            df.loc[len(df)] = [None, None, response_str, path]\n",
    "            metadata[path][\"actual\"] = 0\n",
    "    else:\n",
    "        df.loc[len(df)] = [None, None, None, path]\n",
    "        metadata[path][\"actual\"] = 0\n",
    "\n",
    "display(df.head(2))\n",
    "if os.path.exists(f\"dataset/{legal_name}.csv\"):\n",
    "    shutil.copy(f\"dataset/{legal_name}.csv\", f\"dataset/{legal_name}_backup.csv\")\n",
    "df.to_csv(f\"dataset/{legal_name}.csv\", index=False, encoding=\"utf-8\")\n",
    "with open(f\"dataset/metadata_{legal_name}.json\", 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
    "print(sum([get_n(len(content.split(\"\\n\\n\"))) for content in contents]), len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 10951.19it/s]\n",
      "9it [08:12, 54.73s/it] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>references</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{\\n    1: {\\n        question: \"หากบุคคลใดไม่ป...</td>\n",
       "      <td>_ตรวจแล้ว/พระราชบัญญัติการแข่งขันทางการค้า/พระ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>หากบริษัทฝ่าฝืนมาตรา 51 วรรคหนึ่ง จะต้องชำระค่...</td>\n",
       "      <td>ต้องชำระค่าปรับทางปกครองในอัตราไม่เกินสองแสนบา...</td>\n",
       "      <td>อ้างอิงจากมาตรา 80 ของพระราชบัญญัติการแข่งขันท...</td>\n",
       "      <td>_ตรวจแล้ว/พระราชบัญญัติการแข่งขันทางการค้า/พระ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                                               None   \n",
       "1  หากบริษัทฝ่าฝืนมาตรา 51 วรรคหนึ่ง จะต้องชำระค่...   \n",
       "\n",
       "                                              answer  \\\n",
       "0                                               None   \n",
       "1  ต้องชำระค่าปรับทางปกครองในอัตราไม่เกินสองแสนบา...   \n",
       "\n",
       "                                          references  \\\n",
       "0  {\\n    1: {\\n        question: \"หากบุคคลใดไม่ป...   \n",
       "1  อ้างอิงจากมาตรา 80 ของพระราชบัญญัติการแข่งขันท...   \n",
       "\n",
       "                                              source  \n",
       "0  _ตรวจแล้ว/พระราชบัญญัติการแข่งขันทางการค้า/พระ...  \n",
       "1  _ตรวจแล้ว/พระราชบัญญัติการแข่งขันทางการค้า/พระ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 70\n"
     ]
    }
   ],
   "source": [
    "legal_name = \"พระราชบัญญัติการแข่งขันทางการค้า\"\n",
    "contents, paths = read_dir(f\"_ตรวจแล้ว/{legal_name}\")\n",
    "results = run_in_parallel(contents, paths, num_threads=4)\n",
    "\n",
    "df = pd.DataFrame(columns=['question', 'answer', 'references', 'source'])\n",
    "\n",
    "metadata = {}\n",
    "for i in range(len(contents)):\n",
    "    metadata[paths[i]] = {\"expected\": get_n(len(contents[i].split(\"\\n\\n\"))), \"actual\":0}\n",
    "\n",
    "for response_str, path in results:\n",
    "    if response_str:\n",
    "        start_index = response_str.find('{')\n",
    "        end_index = response_str.rfind('}') + 1\n",
    "        if start_index != -1 and end_index != -1: json_str = response_str[start_index:end_index]\n",
    "        else:\n",
    "            raise Exception(\"JSON not found\")\n",
    "        try:\n",
    "            response = json.loads(json_str)\n",
    "            # print(response_str[8:-4])\n",
    "            counts = 0\n",
    "            for key, value in response.items():\n",
    "                # Extract each question, answer, and references, and add them to the DataFrame\n",
    "                df.loc[len(df)] = [value['question'], value['answer'], value['references'], path]\n",
    "                counts += 1\n",
    "            metadata[path][\"actual\"] = counts\n",
    "        except Exception as e:\n",
    "            df.loc[len(df)] = [None, None, response_str, path]\n",
    "            metadata[path][\"actual\"] = 0\n",
    "    else:\n",
    "        df.loc[len(df)] = [None, None, None, path]\n",
    "        metadata[path][\"actual\"] = 0\n",
    "\n",
    "display(df.head(2))\n",
    "if os.path.exists(f\"dataset/{legal_name}.csv\"):\n",
    "    shutil.copy(f\"dataset/{legal_name}.csv\", f\"dataset/{legal_name}_backup.csv\")\n",
    "df.to_csv(f\"dataset/{legal_name}.csv\", index=False, encoding=\"utf-8\")\n",
    "with open(f\"dataset/metadata_{legal_name}.json\", 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
    "print(sum([get_n(len(content.split(\"\\n\\n\"))) for content in contents]), len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fix errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_json_like_string(json_like_str):\n",
    "    # Regular expression to match keys in the format: key: (without quotes)\n",
    "    # This regex assumes keys are alphanumeric with underscores\n",
    "    regex = r'(?<!\")(\\b\\w+\\b)(?=\\s*:)'  # Look for word characters ending with ':' not preceded by a quote\n",
    "    \n",
    "    # Function to add double quotes around the found keys\n",
    "    def add_quotes(match):\n",
    "        return f'\"{match.group(1)}\"'\n",
    "    \n",
    "    # Replace all occurrences of keys without quotes with quoted keys\n",
    "    fixed_str = re.sub(regex, add_quotes, json_like_str)\n",
    "    \n",
    "    return fixed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in tqdm(glob(os.path.join('dataset', '**/*.json'), recursive=True)):\n",
    "    metadata = json.load(open(file_path, 'r', encoding=\"utf-8\"))\n",
    "    expected = sum([val[\"expected\"] for val in metadata.values()])\n",
    "    actual = len(pd.read_csv(file_path.replace(\".json\", \".csv\").replace(\"metadata/metadata_\",\"\"), encoding=\"utf-8\"))\n",
    "    if expected!=actual:\n",
    "        print(file_path, expected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>references</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\\n    1: {\\n        question: \"หากบุคคลใดไม่ป...</td>\n",
       "      <td>_ตรวจแล้ว/พระราชบัญญัติการแข่งขันทางการค้า/พระ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question answer                                         references  \\\n",
       "0      NaN    NaN  {\\n    1: {\\n        question: \"หากบุคคลใดไม่ป...   \n",
       "\n",
       "                                              source  \n",
       "0  _ตรวจแล้ว/พระราชบัญญัติการแข่งขันทางการค้า/พระ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 77 0\n"
     ]
    }
   ],
   "source": [
    "legal_name = \"พระราชบัญญัติการแข่งขันทางการค้า\"\n",
    "df = pd.read_csv(f\"dataset/{legal_name}.csv\", encoding=\"utf-8\")\n",
    "metadata = json.load(open(f\"dataset/metadata/metadata_{legal_name}.json\", 'r', encoding=\"utf-8\"))\n",
    "\n",
    "condition = df['question'].isna() # | df['answer'].isna() | df['references'].isna()\n",
    "df_filtered = df[~condition].reset_index(drop=True)\n",
    "df_error = df[condition].reset_index(drop=True)\n",
    "unhandle_error = pd.DataFrame(columns=['references', 'source'])\n",
    "\n",
    "display(df_error.head(len(df_error)))\n",
    "\n",
    "for index, row in df_error.iterrows():\n",
    "    path = row[\"source\"]\n",
    "    response_str = row['references']\n",
    "    if isinstance(response_str, float):\n",
    "        with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        response_str, _ = process_content((content, path))\n",
    "        # print(response_str)\n",
    "    tmp = response_str\n",
    "    start_index = response_str.find('{')\n",
    "    end_index = response_str.rfind('}') + 1\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        try:\n",
    "            response_str = tmp\n",
    "            start_index = response_str.find('{')\n",
    "            end_index = response_str.rfind('}') + 1\n",
    "            json_str = response_str[start_index:end_index]\n",
    "            response = json.loads(fix_json_like_string(json_str))\n",
    "            for key, value in response.items():\n",
    "                df_filtered.loc[len(df_filtered)] = [value['question'], value['answer'], value['references'], row[\"source\"]]\n",
    "        except Exception as e:\n",
    "            response_str = tmp\n",
    "            start_index = response_str.find('{')\n",
    "            end_index = response_str.rfind('}') + 1\n",
    "            json_str = response_str[start_index:end_index] + '\\n}'\n",
    "            try:\n",
    "                response = json.loads(fix_json_like_string(json_str))\n",
    "                for key, value in response.items():\n",
    "                    df_filtered.loc[len(df_filtered)] = [value['question'], value['answer'], value['references'], row[\"source\"]]\n",
    "            except Exception as e:\n",
    "                unhandle_error.loc[len(unhandle_error)] = [response_str, path]\n",
    "    else:\n",
    "        unhandle_error.loc[len(unhandle_error)] = [response_str, path]\n",
    "print(sum([val[\"expected\"] for val in metadata.values()]), len(df_filtered), len(unhandle_error))\n",
    "if len(unhandle_error)>0:\n",
    "    unhandle_error.to_csv(f\"dataset/unhandle_error.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 77)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "tmp = df_filtered.copy()\n",
    "tmp = tmp.drop_duplicates(subset=['question'],keep='first').reset_index(drop=True)\n",
    "len(tmp), len(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "if os.path.exists(f\"dataset/{legal_name}.csv\"):\n",
    "    shutil.copy(f\"dataset/{legal_name}.csv\", f\"dataset/{legal_name}_backup.csv\")\n",
    "df_filtered.to_csv(f\"dataset/{legal_name}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in tqdm(glob(os.path.join('dataset', '**/*.csv'), recursive=True)):\n",
    "    if \"backup\" not in file_path:\n",
    "        df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "        tmp = df.copy()\n",
    "        tmp = tmp.drop_duplicates(subset=['question'],keep='first').reset_index(drop=True)\n",
    "        if len(tmp)!=len(df):\n",
    "            shutil.copy(f\"dataset/{legal_name}.csv\", f\"dataset/{legal_name}_backup_w_duplicate.csv\")\n",
    "            tmp.to_csv(f\"dataset/{legal_name}.csv\", index=False, encoding=\"utf-8\")\n",
    "            print(file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Finalize data**\n",
    "Generate dataset.csv & swap rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 800.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4121, 4121)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['question', 'answer', 'references', 'source'])\n",
    "count = 0\n",
    "\n",
    "for file_path in tqdm(glob(os.path.join('dataset', '**/*.csv'), recursive=True)):\n",
    "    if \"backup\" not in file_path:\n",
    "        tmp = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "        count += len(tmp)\n",
    "        df = pd.concat([df, tmp], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.to_csv(f\"dataset.csv\", index=False, encoding=\"utf-8\")\n",
    "len(df), count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
