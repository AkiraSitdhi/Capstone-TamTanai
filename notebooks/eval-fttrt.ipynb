{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "from bert_score import score\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import AutoTokenizer\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"model\")\n",
    "tokenizer.model_max_length = 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trt\n",
    "\n",
    "df_base = pd.read_csv('fttrt/trt-base.csv',encoding='utf-8')\n",
    "df_ir = pd.read_csv('fttrt/trt-ft.csv',encoding='utf-8')\n",
    "df = pd.merge(df_base, df_ir, on=['question','answer','knowledges','prompt'], how='inner')\n",
    "print(len(df))\n",
    "df['time_base'].mean(), df['time_finetune'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft\n",
    "\n",
    "df_base = pd.read_csv('fttrt/ft-base.csv',encoding='utf-8')\n",
    "df_ir = pd.read_csv('fttrt/ft-ft.csv',encoding='utf-8')\n",
    "df = pd.merge(df_base, df_ir, on=['question','answer','knowledges','prompt'], how='inner')\n",
    "print(len(df))\n",
    "df['time_base'].mean(), df['time_finetune'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = sorted(df['time_base'].to_list())\n",
    "ftt = sorted(df['time_finetune'].to_list())\n",
    "trim = int(0.05*len(bt))\n",
    "sum(bt[trim:-trim])/len(bt[trim:-trim]), sum(ftt[trim:-trim])/len(ftt[trim:-trim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2))\n",
    "plt.hist(ftt, bins=10, alpha=0.7, label='Base Model Time')\n",
    "plt.hist(bt, bins=10, alpha=0.7, label='Finetuned Model Time')\n",
    "\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Time Distribution for Base and Fine-tuned Models')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trt from ir\n",
    "\n",
    "df_base = pd.read_csv('fttrt/trt-ir-base.csv',encoding='utf-8')\n",
    "df_ir = pd.read_csv('fttrt/trt-ir-ir.csv',encoding='utf-8')\n",
    "df = pd.merge(df_base, df_ir, on=['question','answer','knowledges','prompt'], how='inner')\n",
    "print(len(df))\n",
    "df['time_base'].mean(), df['time_finetune'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert score trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'microsoft/deberta-xlarge-mnli'  \n",
    "\n",
    "labels = df[\"answer\"].to_list()\n",
    "b = df[\"response_base\"].to_list()\n",
    "ft = df[\"response_finetune\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('inference base')\n",
    "P, R, F1 = score(b, labels, model_type=model_type, verbose=False)\n",
    "print('precision\\t', P.mean().item())\n",
    "print('recall\\t', R.mean().item())\n",
    "print('f1\\t', F1.mean().item())\n",
    "\n",
    "print('inference finetune')\n",
    "P, R, F1 = score(ft, labels, model_type=model_type, verbose=False)\n",
    "print('precision\\t', P.mean().item())\n",
    "print('recall\\t', R.mean().item())\n",
    "print('f1\\t', F1.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert score ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'microsoft/deberta-xlarge-mnli'  \n",
    "\n",
    "labels = df[\"answer\"].to_list()\n",
    "b = df[\"response_base\"].to_list()\n",
    "ft = df[\"response_finetune\"].to_list()\n",
    "\n",
    "print('inference base')\n",
    "P, R, F1 = score(b, labels, model_type=model_type, verbose=False)\n",
    "print('precision\\t', P.mean().item())\n",
    "print('recall\\t', R.mean().item())\n",
    "print('f1\\t', F1.mean().item())\n",
    "\n",
    "print('inference finetune')\n",
    "P, R, F1 = score(ft, labels, model_type=model_type, verbose=False)\n",
    "print('precision\\t', P.mean().item())\n",
    "print('recall\\t', R.mean().item())\n",
    "print('f1\\t', F1.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert score trt (ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'microsoft/deberta-xlarge-mnli'  \n",
    "\n",
    "labels = df[\"answer\"].to_list()\n",
    "b = df[\"response_base\"].to_list()\n",
    "ft = df[\"response_finetune\"].to_list()\n",
    "\n",
    "print('inference base')\n",
    "P, R, F1 = score(b, labels, lang=\"th\", verbose=False, nthreads=8)\n",
    "print('precision\\t', P.mean().item())\n",
    "print('recall\\t', R.mean().item())\n",
    "print('f1\\t', F1.mean().item())\n",
    "\n",
    "print('inference finetune')\n",
    "P, R, F1 = score(ft, labels,lang=\"th\",verbose=False, nthreads=8)\n",
    "print('precision\\t', P.mean().item())\n",
    "print('recall\\t', R.mean().item())\n",
    "print('f1\\t', F1.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rouge score trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"answer\"].to_list()\n",
    "bs = df[\"response_base\"].to_list()\n",
    "fts= df[\"response_finetune\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True, tokenizer=tokenizer)\n",
    "fb = [0,0,0]\n",
    "fft = [0,0,0]\n",
    "\n",
    "for label, b, ft in zip(labels, bs, fts):\n",
    "    scores = scorer.score(label, b)\n",
    "    fb[0] += scores['rouge1'].fmeasure\n",
    "    fb[1] += scores['rouge2'].fmeasure\n",
    "    fb[2] += scores['rougeL'].fmeasure\n",
    "\n",
    "    scores = scorer.score(label, ft)\n",
    "    fft[0] += scores['rouge1'].fmeasure\n",
    "    fft[1] += scores['rouge2'].fmeasure\n",
    "    fft[2] += scores['rougeL'].fmeasure\n",
    "\n",
    "for i in range(3):\n",
    "    fb[i] /= len(labels)\n",
    "    fft[i] /= len(labels)\n",
    "\n",
    "fb, fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rouge score ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"tmps/inference_base.csv\", encoding='utf-8')\n",
    "df2 = pd.read_csv(\"tmps/inference_finetune.csv\", encoding='utf-8')\n",
    "df = pd.merge(df1, df2, on=['question','answer','references','knowledges', 'source'], how='inner')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"answer\"].to_list()\n",
    "bs = df[\"response_base\"].to_list()\n",
    "fts= df[\"response_finetune\"].to_list()\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True, tokenizer=tokenizer)\n",
    "fb = [0,0,0]\n",
    "fft = [0,0,0]\n",
    "\n",
    "for label, b, ft in zip(labels, bs, fts):\n",
    "    scores = scorer.score(label, b)\n",
    "    fb[0] += scores['rouge1'].fmeasure\n",
    "    fb[1] += scores['rouge2'].fmeasure\n",
    "    fb[2] += scores['rougeL'].fmeasure\n",
    "\n",
    "    scores = scorer.score(label, ft)\n",
    "    fft[0] += scores['rouge1'].fmeasure\n",
    "    fft[1] += scores['rouge2'].fmeasure\n",
    "    fft[2] += scores['rougeL'].fmeasure\n",
    "\n",
    "for i in range(3):\n",
    "    fb[i] /= len(labels)\n",
    "    fft[i] /= len(labels)\n",
    "\n",
    "fb, fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bleu score trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"model\")\n",
    "tokenizer.model_max_length = 1000000000\n",
    "\n",
    "labels = df[\"answer\"].to_list()\n",
    "bs = df[\"response_base\"].to_list()\n",
    "fts= df[\"response_finetune\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = []\n",
    "bft = []\n",
    "\n",
    "for label, b, ft in zip(labels, bs, fts):\n",
    "\n",
    "    label = tokenizer.encode(label, add_special_tokens=False)\n",
    "    b = tokenizer.encode(b, add_special_tokens=False)\n",
    "    ft = tokenizer.encode(ft, add_special_tokens=False)\n",
    "    label = [tokenizer.decode(l, skip_special_tokens=True) for l in label]\n",
    "    b = [tokenizer.decode(l, skip_special_tokens=True) for l in b]\n",
    "    ft = [tokenizer.decode(l, skip_special_tokens=True) for l in ft]\n",
    "\n",
    "    bb.append(sentence_bleu([label], b))\n",
    "    bft.append(sentence_bleu([label], ft))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(bb)/len(bb), sum(bft)/len(bft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bleu score ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"model\")\n",
    "tokenizer.model_max_length = 1000000000\n",
    "\n",
    "labels = df[\"answer\"].to_list()\n",
    "bs = df[\"response_base\"].to_list()\n",
    "fts= df[\"response_finetune\"].to_list()\n",
    "\n",
    "bb = []\n",
    "bft = []\n",
    "\n",
    "for label, b, ft in zip(labels, bs, fts):\n",
    "\n",
    "    label = tokenizer.encode(label, add_special_tokens=False)\n",
    "    b = tokenizer.encode(b, add_special_tokens=False)\n",
    "    ft = tokenizer.encode(ft, add_special_tokens=False)\n",
    "    label = list(set([tokenizer.decode(l, skip_special_tokens=True) for l in label]))\n",
    "    b = list(set([tokenizer.decode(l, skip_special_tokens=True) for l in b]))\n",
    "    ft = list(set([tokenizer.decode(l, skip_special_tokens=True) for l in ft]))\n",
    "\n",
    "    bb.append(sentence_bleu([label], b, weights=[1,0,0,0]))\n",
    "    bft.append(sentence_bleu([label], ft, weights=[1,0,0,0]))\n",
    "\n",
    "sum(bb)/len(bb), sum(bft)/len(bft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
