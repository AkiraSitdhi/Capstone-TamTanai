{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from time import time as ttt\n",
    "from tqdm import tqdm\n",
    "load_dotenv()\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "from bert_score import score\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import AutoTokenizer\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"model\")\n",
    "tokenizer.model_max_length = 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('testdata.csv',encoding='utf-8')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(question):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "def ask_gpt(\n",
    "    question,\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "):\n",
    "    try:\n",
    "        ttti = ttt()\n",
    "        messages = gen_prompt(question)\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            # default settings\n",
    "            temperature=0,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            # stop=stop_sequence,\n",
    "        )\n",
    "        tttf = ttt()\n",
    "        return [True, response.choices[0].message.content, tttf-ttti]\n",
    "    except Exception as e:\n",
    "        return [False, e, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "times = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    question = row['question']\n",
    "    status, response, time = ask_gpt(question)\n",
    "    if status:\n",
    "        answers.append(response)\n",
    "        times.append(time)\n",
    "    else:\n",
    "        print(f\"{index} failed: {response}\")\n",
    "        answers.append(None)\n",
    "        times.append(None)\n",
    "\n",
    "df['response'] = answers\n",
    "df['time'] = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('e2e-gpt.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('e2e/e2e-gpt.csv',encoding='utf-8')\n",
    "print(len(df))\n",
    "df['time'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ours time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = pd.read_csv(\"ir/ir-ir.csv\",encoding='utf-8')[\"irt\"].to_list()\n",
    "ft = pd.read_csv(\"fttrt/trt-ft.csv\",encoding='utf-8')[\"time_finetune\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert score gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'microsoft/deberta-xlarge-mnli'  \n",
    "\n",
    "labels = df[\"answer\"].to_list()\n",
    "b = df[\"response\"].to_list()\n",
    "\n",
    "print('inference gpt-3.5')\n",
    "P, R, F1 = score(b, labels, model_type=model_type, verbose=False)\n",
    "print('precision\\t', P.mean().item())\n",
    "print('recall\\t', R.mean().item())\n",
    "print('f1\\t', F1.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rouge score gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"answer\"].to_list()\n",
    "bs = df[\"response\"].to_list()\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True, tokenizer=tokenizer)\n",
    "fb = [0,0,0]\n",
    "\n",
    "for label, b in zip(labels, bs):\n",
    "    scores = scorer.score(label, b)\n",
    "    fb[0] += scores['rouge1'].fmeasure\n",
    "    fb[1] += scores['rouge2'].fmeasure\n",
    "    fb[2] += scores['rougeL'].fmeasure\n",
    "\n",
    "for i in range(3):\n",
    "    fb[i] /= len(labels)\n",
    "\n",
    "fb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert score tanoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if row[\"time\"]==300:\n",
    "        df.loc[index, \"time\"] = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('e2e/e2e-tanoy.csv',encoding='utf-8')\n",
    "print(len(df))\n",
    "df['time'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'microsoft/deberta-xlarge-mnli'  \n",
    "\n",
    "labels = df[\"answer\"].to_list()\n",
    "b = df[\"response_clean\"].to_list()\n",
    "\n",
    "print('inference tanoy')\n",
    "P, R, F1 = score(b, labels, model_type=model_type, verbose=False)\n",
    "print('precision\\t', P.mean().item())\n",
    "print('recall\\t', R.mean().item())\n",
    "print('f1\\t', F1.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rouge score tanoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"answer\"].to_list()\n",
    "bs = df[\"response_clean\"].to_list()\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True, tokenizer=tokenizer)\n",
    "fb = [0,0,0]\n",
    "\n",
    "for label, b in zip(labels, bs):\n",
    "    scores = scorer.score(label, b)\n",
    "    fb[0] += scores['rouge1'].fmeasure\n",
    "    fb[1] += scores['rouge2'].fmeasure\n",
    "    fb[2] += scores['rougeL'].fmeasure\n",
    "\n",
    "for i in range(3):\n",
    "    fb[i] /= len(labels)\n",
    "\n",
    "fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
